{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    279\n",
      "1     44\n",
      "0     36\n",
      "2     33\n",
      "Name: biome, dtype: int64\n",
      "Acc: 0.6075949367088608\n",
      "Fitting 3 folds for each of 2736 candidates, totalling 8208 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1272 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2002 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2892 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3946 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5160 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6538 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8076 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': {1: 1, 0: 4}, 'criterion': 'entropy', 'max_depth': 6, 'max_features': 'log2', 'n_estimators': 10}\n",
      "0.770346447445684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 8208 out of 8208 | elapsed:  7.1min finished\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pandas.read_csv(\"phase2_827.csv\")\n",
    "\n",
    "X = data.drop(columns=[\"biome\"]).fillna(0)\n",
    "y = data['biome'].map({\"fermitation\":0,\"gut\":1,\"lake\":2,\"soil\":3}).fillna(0).astype(int)\n",
    "\n",
    "# 各个类别的数量\n",
    "print(y.value_counts())\n",
    "\n",
    "# 数据分割，80%训练集，20%测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 随机森林分类\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=1, random_state=0,  class_weight={1:1, 0:2})\n",
    "clf.fit(X_train, y_train)\n",
    "acc = sum(y_test == clf.predict(X_test)) / len(y_test)\n",
    "print(\"Acc:\", acc)\n",
    "\n",
    "\n",
    "\n",
    "# 使用grid search找出最优参数组合\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [i*10 for i in range(1,20)],\n",
    "    'criterion':['entropy', 'entropy'],\n",
    "    'max_depth': [i for i in range(1, 10)],\n",
    "    'max_features': [\"auto\", \"log2\"],\n",
    "    'class_weight':[{1:1, 0:i} for i in range(1,5)]\n",
    "}\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=clf, \n",
    "                           param_grid=param_grid,\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 2)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 最优性能\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    505\n",
      "0     69\n",
      "2     60\n",
      "1     59\n",
      "Name: biome, dtype: int64\n",
      "Acc: 0.6906474820143885\n",
      "Fitting 3 folds for each of 2736 candidates, totalling 8208 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1272 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2002 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2892 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3946 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5160 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6538 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8076 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8208 out of 8208 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': {1: 1, 0: 2}, 'criterion': 'entropy', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 80}\n",
      "0.7619047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=9, n_estimators=10,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pandas.read_csv(\"output1213.csv\")\n",
    "\n",
    "X = data.drop(columns=[\"biome\"]).fillna(0)\n",
    "y = data['biome'].map({\"fermitation\":0,\"gut\":1,\"lake\":2,\"soil\":3}).fillna(0).astype(int)\n",
    "\n",
    "# 各个类别的数量\n",
    "print(y.value_counts())\n",
    "\n",
    "# 数据分割，80%训练集，20%测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 随机森林分类\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=1, random_state=0,  class_weight={1:1, 0:2})\n",
    "clf.fit(X_train, y_train)\n",
    "acc = sum(y_test == clf.predict(X_test)) / len(y_test)\n",
    "print(\"Acc:\", acc)\n",
    "\n",
    "\n",
    "\n",
    "# 使用grid search找出最优参数组合\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [i*10 for i in range(1,20)],\n",
    "    'criterion':['entropy', 'entropy'],\n",
    "    'max_depth': [i for i in range(1, 10)],\n",
    "    'max_features': [\"auto\", \"log2\"],\n",
    "    'class_weight':[{1:1, 0:i} for i in range(1,5)]\n",
    "}\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=clf, \n",
    "                           param_grid=param_grid,\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 2)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 最优性能\n",
    "print(grid_search.best_score_)\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=9,criterion=\"entropy\",\n",
    "    max_features=\"auto\", random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=9, n_estimators=10,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=9,criterion=\"entropy\",\n",
    "    max_features=\"auto\", random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-50edad8d58cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: predict() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "\n",
    "data2=pandas.read_csv(\"output_knowlevel4.csv\")\n",
    "y_train = data2[\"pfam\"]\n",
    "X_test = data2.drop(columns=[\"pfam\"]).fillna(0)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_train = label_binarize(y_train, classes=[0, 1, 2, 3])\n",
    "X_test = X_test / X_test.sum()\n",
    "y_score = clf.predict(X_test,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 0 0 1 0 0 0 0 0 0 3 1 3 3 1 3 3 3 3 3 3 0 1 3 1 1 1 1 1 0 1 3 3 1 3\n",
      " 1 3 3 1 3 3 3 3 3 2 2 3 2 3 3 2 2 3 2 2 2 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    505\n",
      "0     69\n",
      "2     60\n",
      "1     59\n",
      "Name: biome, dtype: int64\n",
      "Acc: 0.7553956834532374\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 145 and input n_features is 199 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8818a45cb26a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0my_vali\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_vali\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mx_vali\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_vali\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mx_vali\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vali\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             raise ValueError(\"Number of features of the model must \"\n\u001b[0m\u001b[0;32m    397\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 145 and input n_features is 199 "
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "\n",
    "data = pandas.read_csv(\"output1213.csv\")\n",
    "\n",
    "X = data.drop(columns=[\"biome\"]).fillna(0)\n",
    "y = data['biome'].map({\"fermitation\":0,\"gut\":1,\"lake\":2,\"soil\":3}).fillna(0).astype(int)\n",
    "\n",
    "# 各个类别的数量\n",
    "print(y.value_counts())\n",
    "\n",
    "# 数据分割，80%训练集，20%测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 随机森林分类\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=9,criterion=\"entropy\",\n",
    "    max_features=\"auto\", random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "acc = sum(y_test == clf.predict(X_test)) / len(y_test)\n",
    "print(\"Acc:\", acc)\n",
    "\n",
    "data2=pandas.read_csv(\"know_model.csv\")\n",
    "y_vali = data2[\"pfam\"]\n",
    "x_vali = data2.drop(columns=[\"pfam\"]).fillna(0)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_vali = le.fit_transform(y_vali)\n",
    "y_vali = label_binarize(y_vali, classes=[0, 1, 2, 3])\n",
    "x_vali = x_vali / x_vali.sum()\n",
    "a=clf.predict_proba(x_vali)\n",
    "numpy.savetxt('test.txt',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
